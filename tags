!_TAG_FILE_FORMAT	2	/extended format; --format=1 will not append ;" to lines/
!_TAG_FILE_SORTED	1	/0=unsorted, 1=sorted, 2=foldcase/
!_TAG_PROGRAM_AUTHOR	Darren Hiebert	/dhiebert@users.sourceforge.net/
!_TAG_PROGRAM_NAME	Exuberant Ctags	//
!_TAG_PROGRAM_URL	http://ctags.sourceforge.net	/official site/
!_TAG_PROGRAM_VERSION	5.8	//
F	models/decoder.py	/^import torch.nn.functional as F$/;"	i
F	models/encoder.py	/^import torch.nn.functional as F$/;"	i
F	models/multihead_attention.py	/^import torch.nn.functional as F$/;"	i
F	models/positional_embedding.py	/^import torch.nn.functional as F$/;"	i
F	models/transformer.py	/^import torch.nn.functional as F$/;"	i
LearnedPositionalEmbedding	models/positional_embedding.py	/^class LearnedPositionalEmbedding(nn.Embedding):$/;"	c
Linear	models/encoder.py	/^from .utils import Linear$/;"	i
Linear	models/utils.py	/^def Linear(in_features, out_features, bias=True):$/;"	f
MultiheadAttn	models/decoder.py	/^from .multihead_attention import MultiheadAttn$/;"	i
MultiheadAttn	models/encoder.py	/^from .multihead_attention import MultiheadAttn$/;"	i
MultiheadAttn	models/multihead_attention.py	/^class MultiheadAttn(nn.Module):$/;"	c
Parameter	models/multihead_attention.py	/^from  torch.nn import Parameter$/;"	i
SinusoidalPositionalEmbedding	models/positional_embedding.py	/^class SinusoidalPositionalEmbedding(nn.Module):$/;"	c
Transformer	models/transformer.py	/^class Transformer(nn.Module):$/;"	c
TransformerDecoder	models/decoder.py	/^class TransformerDecoder(nn.Module):$/;"	c
TransformerDecoder	models/transformer.py	/^from .decoder import TransformerDecoder$/;"	i
TransformerDecoderLayer	models/decoder.py	/^class TransformerDecoderLayer(nn.Module):$/;"	c
TransformerEncoder	models/encoder.py	/^class TransformerEncoder(nn.Module):$/;"	c
TransformerEncoder	models/transformer.py	/^from .encoder import TransformerEncoder$/;"	i
TransformerEncoderLayer	models/encoder.py	/^class TransformerEncoderLayer(nn.Module):$/;"	c
TranslationLM	models/transformer.py	/^class TranslationLM(TransformerDecoder):$/;"	c
__init__	models/decoder.py	/^    def __init__(self, args, no_encoder_attn=False):$/;"	m	class:TransformerDecoderLayer
__init__	models/decoder.py	/^    def __init__(self, args, vocabsize, pad_idx, no_enc_attn=False):$/;"	m	class:TransformerDecoder
__init__	models/encoder.py	/^    def __init__(self, args):$/;"	m	class:TransformerEncoderLayer
__init__	models/encoder.py	/^    def __init__(self, args, vocabsize, pad_idx):$/;"	m	class:TransformerEncoder
__init__	models/multihead_attention.py	/^    def __init__(self, embed_dim, n_heads,kdim=None, vdim=None, dout=0.0, $/;"	m	class:MultiheadAttn
__init__	models/positional_embedding.py	/^    def __init__(self, embed_dim, pad_idx, init_size=1024):$/;"	m	class:SinusoidalPositionalEmbedding
__init__	models/positional_embedding.py	/^    def __init__(self, num_emb, emb_dim, padding_idx):$/;"	m	class:LearnedPositionalEmbedding
__init__	models/transformer.py	/^    def __init__(self, args, vocabsize, pad_idx, bos_idx, sep_idx):$/;"	m	class:TranslationLM
__init__	models/transformer.py	/^    def __init__(self, encoder, decoder, bos_idx):$/;"	m	class:Transformer
_in_projection	models/multihead_attention.py	/^    def _in_projection(self, input, start=0, end=None):$/;"	m	class:MultiheadAttn
buffered_future_mask	models/decoder.py	/^    def buffered_future_mask(self, tensor):$/;"	m	class:TransformerDecoder
buffered_future_mask	models/transformer.py	/^    def buffered_future_mask(self, tensor, delim_idx=1):$/;"	m	class:TranslationLM
fill_ninf	models/transformer.py	/^from .utils import fill_ninf$/;"	i
fill_ninf	models/utils.py	/^def fill_ninf(t):$/;"	f
forward	models/decoder.py	/^    def forward(self, prev_tokens, enc_outs, incremental_state=None):$/;"	m	class:TransformerDecoder
forward	models/decoder.py	/^    def forward(self, x, enc_out, enc_pad_mask, attn_mask, dec_pad_mask, incremental_state=None):$/;"	m	class:TransformerDecoderLayer
forward	models/encoder.py	/^    def forward(self, inputs):$/;"	m	class:TransformerEncoder
forward	models/encoder.py	/^    def forward(self, x, encoder_pad_mask):$/;"	m	class:TransformerEncoderLayer
forward	models/multihead_attention.py	/^    def forward(self, query, key, value, key_pad_mask=None, incremental_state=None, $/;"	m	class:MultiheadAttn
forward	models/positional_embedding.py	/^    def forward(self, input, incremental_state=None):$/;"	m	class:LearnedPositionalEmbedding
forward	models/positional_embedding.py	/^    def forward(self, input, incremental_state=None, timestep=None):$/;"	m	class:SinusoidalPositionalEmbedding
forward	models/transformer.py	/^    def forward(self, srcs, prev_tokens):$/;"	m	class:Transformer
forward	models/transformer.py	/^    def forward(self, srcs, tgts=None, incremental_state=None):$/;"	m	class:TranslationLM
generate	models/transformer.py	/^    def generate(self, srcs, maxlen):$/;"	m	class:Transformer
generate	models/transformer.py	/^    def generate(self, srcs, maxlen):$/;"	m	class:TranslationLM
get_embedding	models/positional_embedding.py	/^    def get_embedding(num_emb, embed_dim, pad_idx=None):$/;"	m	class:SinusoidalPositionalEmbedding
in_projection_k	models/multihead_attention.py	/^    def in_projection_k(self, key):$/;"	m	class:MultiheadAttn
in_projection_q	models/multihead_attention.py	/^    def in_projection_q(self, query):$/;"	m	class:MultiheadAttn
in_projection_qkv	models/multihead_attention.py	/^    def in_projection_qkv(self, query):$/;"	m	class:MultiheadAttn
in_projection_v	models/multihead_attention.py	/^    def in_projection_v(self, value):$/;"	m	class:MultiheadAttn
make_positions	models/positional_embedding.py	/^def make_positions(tensor, pad_idx):$/;"	f
math	models/decoder.py	/^import math$/;"	i
math	models/encoder.py	/^import math$/;"	i
math	models/positional_embedding.py	/^import math$/;"	i
math	models/transformer.py	/^import math$/;"	i
maybe_normalize	models/decoder.py	/^    def maybe_normalize(self, layer_norm, x, before=False, after=False):$/;"	m	class:TransformerDecoderLayer
maybe_normalize	models/encoder.py	/^    def maybe_normalize(self, layer_norm, x, before=False, after=False):$/;"	m	class:TransformerEncoderLayer
nn	models/decoder.py	/^import torch.nn as nn$/;"	i
nn	models/decoder.py	/^import torch.nn.functional as F$/;"	i
nn	models/encoder.py	/^import torch.nn as nn$/;"	i
nn	models/encoder.py	/^import torch.nn.functional as F$/;"	i
nn	models/multihead_attention.py	/^import torch.nn as nn$/;"	i
nn	models/multihead_attention.py	/^import torch.nn.functional as F$/;"	i
nn	models/positional_embedding.py	/^import torch.nn as nn$/;"	i
nn	models/positional_embedding.py	/^import torch.nn.functional as F$/;"	i
nn	models/transformer.py	/^import torch.nn as nn$/;"	i
nn	models/transformer.py	/^import torch.nn.functional as F$/;"	i
nn	models/utils.py	/^import torch.nn as nn$/;"	i
random	models/transformer.py	/^import random$/;"	i
reset_parameters	models/multihead_attention.py	/^    def reset_parameters(self):$/;"	m	class:MultiheadAttn
torch	models/decoder.py	/^import torch$/;"	i
torch	models/decoder.py	/^import torch.nn as nn$/;"	i
torch	models/decoder.py	/^import torch.nn.functional as F$/;"	i
torch	models/encoder.py	/^import torch$/;"	i
torch	models/encoder.py	/^import torch.nn as nn$/;"	i
torch	models/encoder.py	/^import torch.nn.functional as F$/;"	i
torch	models/multihead_attention.py	/^import torch$/;"	i
torch	models/multihead_attention.py	/^import torch.nn as nn$/;"	i
torch	models/multihead_attention.py	/^import torch.nn.functional as F$/;"	i
torch	models/positional_embedding.py	/^import torch$/;"	i
torch	models/positional_embedding.py	/^import torch.nn as nn$/;"	i
torch	models/positional_embedding.py	/^import torch.nn.functional as F$/;"	i
torch	models/transformer.py	/^import torch$/;"	i
torch	models/transformer.py	/^import torch.nn as nn$/;"	i
torch	models/transformer.py	/^import torch.nn.functional as F$/;"	i
torch	models/utils.py	/^import torch$/;"	i
torch	models/utils.py	/^import torch.nn as nn$/;"	i
